{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284b6838-89a5-429d-8688-8c99714863d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "mnist_data = datasets.FashionMNIST('data', train=True, download=False, transform=transforms.ToTensor())\n",
    "mnist_data = list(mnist_data)[:4096*2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d04964-42eb-496c-b039-7a8880442570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eigen_init_(n_units, std=1):\n",
    "    sampler = torch.distributions.Normal(torch.Tensor([0]), torch.Tensor([std/n_units]))\n",
    "    Omega = sampler.sample((n_units, n_units))[..., 0]  \n",
    "    w, v = np.linalg.eig(Omega.cpu().detach().numpy())\n",
    "    w =np.random.uniform(-1,1, w.shape[0])\n",
    "    return torch.from_numpy(reconstruct_operator(w,v).real).float()\n",
    "\n",
    "def reconstruct_operator(w, v):\n",
    "    \"\"\"\n",
    "    Recreate a matrix from its eigenvalues and eigenvectors.\n",
    "    \"\"\"\n",
    "    R = np.linalg.inv(v)\n",
    "    # create diagonal matrix from eigenvalues\n",
    "    L = np.diag(w)\n",
    "    # reconstruct the original matrix\n",
    "    B = v.dot(L).dot(R)\n",
    "    return B\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, eigen_init=False, b=16):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential( # like the Composition layer you built\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 7)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(64, b),\n",
    "            nn.Linear(b, b),\n",
    "            nn.Linear(b, 64)\n",
    "        )\n",
    "        if eigen_init:\n",
    "            self.linear[2].weight.data = eigen_init_(b, std=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(-1, 64)\n",
    "        x = self.linear(x)\n",
    "        x = torch.reshape(x, (x.size(0), 64, 1, 1))\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70473cae-95eb-49c4-a63c-e11b04147967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs=5, batch_size=64, learning_rate=1e-3):\n",
    "    torch.manual_seed(42)\n",
    "    criterion = nn.MSELoss() # mean square error loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=learning_rate, \n",
    "                                 weight_decay=1e-5) # <--\n",
    "    train_loader = torch.utils.data.DataLoader(mnist_data, \n",
    "                                               batch_size=batch_size, \n",
    "                                               shuffle=True)\n",
    "    outputs, losses = [], []\n",
    "    for epoch in range(num_epochs):\n",
    "        for data in train_loader:\n",
    "            img, _ = data\n",
    "            recon = model(img)\n",
    "            loss = criterion(recon, img)\n",
    "            losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        print('Epoch:{}, Loss:{:.4f}'.format(epoch+1, float(loss)))\n",
    "        outputs.append((epoch, img, recon),)\n",
    "    return outputs, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec52d11f-d49d-484e-847c-c742456a38ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder(eigen_init=True, b=64)\n",
    "max_epochs = 20\n",
    "outputs, losses = train(model, num_epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0299d57-7e0c-456d-b670-fb71d8bc0c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(0, max_epochs, 5):\n",
    "    plt.figure(figsize=(9, 2))\n",
    "    imgs = outputs[k][1].detach().numpy()\n",
    "    recon = outputs[k][2].detach().numpy()\n",
    "    for i, item in enumerate(imgs):\n",
    "        if i >= 9: break\n",
    "        plt.subplot(2, 9, i+1)\n",
    "        plt.imshow(item[0])\n",
    "        \n",
    "    for i, item in enumerate(recon):\n",
    "        if i >= 9: break\n",
    "        plt.subplot(2, 9, 9+i+1)\n",
    "        plt.imshow(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37499a19-e2f4-44f4-860e-fdf353141d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = outputs[max_epochs-1][1].detach().numpy()\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(imgs[0][0])\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(imgs[8][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78266ab-ebf5-44c6-8470-3e9655fb12dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = outputs[max_epochs-1][1][0,:,:,:] # first image\n",
    "x2 = outputs[max_epochs-1][1][8,:,:,:] # second image\n",
    "x = torch.stack([x1,x2])     # stack them together so we only call `encoder` once\n",
    "embedding = model.encoder(x)\n",
    "e1 = embedding[0] # embedding of first image\n",
    "e2 = embedding[1] # embedding of second image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe63970-d4ce-4b81-a738-1065430dbff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_values = []\n",
    "for i in range(0, 10):\n",
    "    e = e1 * (i/10) + e2 * (10-i)/10\n",
    "    embedding_values.append(e)\n",
    "embedding_values = torch.stack(embedding_values)\n",
    "\n",
    "recons = model.decoder(embedding_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479aeae6-85e9-4a79-b0f5-9e2f2d5e0916",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 2))\n",
    "for i, recon in enumerate(recons.detach().numpy()):\n",
    "    plt.subplot(2,10,i+1)\n",
    "    plt.imshow(recon[0])\n",
    "plt.subplot(2,10,11)\n",
    "plt.imshow(imgs[8][0])\n",
    "plt.subplot(2,10,20)\n",
    "plt.imshow(imgs[0][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c4275d-3bf9-4721-a013-29412dd25ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(index1, index2):\n",
    "    x1 = mnist_data[index1][0]\n",
    "    x2 = mnist_data[index2][0]\n",
    "    x = torch.stack([x1,x2])\n",
    "    embedding = model.encoder(x)\n",
    "    e1 = embedding[0] # embedding of first image\n",
    "    e2 = embedding[1] # embedding of second image\n",
    "\n",
    "\n",
    "    embedding_values = []\n",
    "    for i in range(0, 10):\n",
    "        e = e1 * (i/10) + e2 * (10-i)/10\n",
    "        embedding_values.append(e)\n",
    "    embedding_values = torch.stack(embedding_values)\n",
    "\n",
    "    recons = model.decoder(embedding_values)\n",
    "\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    for i, recon in enumerate(recons.detach().numpy()):\n",
    "        plt.subplot(2,10,i+1)\n",
    "        plt.imshow(recon[0])\n",
    "    plt.subplot(2,10,11)\n",
    "    plt.imshow(x2[0])\n",
    "    plt.subplot(2,10,20)\n",
    "    plt.imshow(x1[0])\n",
    "\n",
    "interpolate(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c592ed4d-014e-422e-8ad7-24e9b4c16210",
   "metadata": {},
   "source": [
    "# Principal components analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8e52a2-dd64-4fe4-b5e5-08e43cbdcbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "for img, labels in mnist_data:\n",
    "    items.append(img.squeeze(0))\n",
    "items = np.array(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a05446-0226-4962-931f-f06ad75a76e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_fashion = np.average(mnist_data, axis=0)[0]\n",
    "plt.imshow(average_fashion.squeeze(0));\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(mnist_data, \n",
    "                                           batch_size=len(mnist_data), \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9deb817-791b-4f21-81af-f944af57d4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = next(iter(train_loader))\n",
    "imgs = imgs.reshape((imgs.size(0), 28*28))\n",
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd39d6f0-5b60-453e-ad22-ae168121e11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_784 = PCA(n_components=784)\n",
    "pca_784.fit(imgs)\n",
    "\n",
    "plt.grid()\n",
    "plt.plot(np.cumsum(pca_784.explained_variance_ratio_ * 100))\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Explained variance')\n",
    "plt.savefig('Scree plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756d0fce-55a6-4d49-84c6-edc8b6ea39ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_10 = PCA(n_components=10)\n",
    "mnist_pca_10_reduced = pca_10.fit_transform(imgs)\n",
    "mnist_pca_10_recovered = pca_10.inverse_transform(mnist_pca_10_reduced)\n",
    "\n",
    "image_pca_10 = mnist_pca_10_recovered[1,:].reshape([28,28])\n",
    "plt.imshow(image_pca_10)\n",
    "plt.title('Compressed image with 10 components', fontsize=15, pad=15)\n",
    "plt.savefig(\"image_pca_10.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0ede34-1a5a-472f-af65-d66db41dd12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_184 = PCA(n_components=184)\n",
    "mnist_pca_184_reduced = pca_184.fit_transform(imgs)\n",
    "mnist_pca_184_recovered = pca_184.inverse_transform(mnist_pca_184_reduced)\n",
    "\n",
    "image_pca_184 = mnist_pca_184_recovered[1,:].reshape([28,28])\n",
    "plt.imshow(image_pca_184)\n",
    "plt.title('Compressed image with 184 components', fontsize=15, pad=15)\n",
    "plt.savefig(\"image_pca_184.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3aafdb-6e54-4241-bf87-23f8d5840ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_10.get_covariance().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274070dc-4e04-4f6a-a6b3-d3170353a17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explained variance of 184 components\n",
    "np.cumsum(pca_184.explained_variance_ratio_ * 100)[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis3-22.04]",
   "language": "python",
   "name": "conda-env-analysis3-22.04-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
